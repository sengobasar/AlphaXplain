# â™Ÿï¸ AlphaXplain â€” Explainable AlphaZero Chess Engine

> *AlphaZero + Explainability. A chess engine that doesn't just play â€” it thinks out loud.*

```
AlphaXplain = AlphaZero-style Self-Play RL + MCTS + Structured LLM Reasoning
```
<img width="1917" height="1011" alt="image" src="https://github.com/user-attachments/assets/98e6715c-1548-4106-9be1-1d6b4b6acd3e" />



---

## âš ï¸ Honest Disclaimer

> **Due to limited compute, I trained for ~30 iterations (~0.5 hours on a consumer CPU/GPU).**
> Full-scale AlphaZero required thousands of Google TPUs running for weeks.
>
> **This means the engine plays terribly â€” and that's completely expected.**
> The goal of AlphaXplain is not superhuman chess. It's to demonstrate the *algorithmic architecture* of a self-learning, self-explaining AI system in a resource-efficient, fully local environment.
>
> Think of it as AlphaZero's brain transplanted into a laptop. The wiring is the same. The muscles just haven't trained yet.

---

## ğŸ§  What Is AlphaXplain?

AlphaXplain is a chess-playing AI that learns entirely from self-play â€” no human games, no handcrafted rules â€” and then explains its own reasoning in plain English using a local language model.

I built it on three ideas that rarely meet in one project:

1. **AlphaZero-style reinforcement learning** â€” the engine teaches itself by playing against itself, guided only by wins and losses.
2. **Monte Carlo Tree Search (MCTS)** â€” instead of brute-force calculation, it explores the future intelligently, using its neural network as a guide.
3. **Structured LLM Explanation** â€” after each move, MCTS statistics (visit counts, Q-values, policy probabilities) are passed to a local language model (Phi via Ollama), which converts raw numbers into human-readable reasoning.

This combination produces what is rarely seen in game-playing AI: **an engine that can explain why it made a decision.**

---

## ğŸŸ¡ Novel Contribution: Grounded Chain-of-Thought Extraction

Most chess engines are black boxes. You see the move â€” not the reasoning.

AlphaXplain introduces a **structured reasoning extraction layer**: MCTS statistics are serialized into a structured format and passed to a local LLM (Phi via Ollama). The model generates natural language explanation grounded in actual search data â€” not hallucination. This is what researchers call a **faithful explanation pipeline**.

To my knowledge, few publicly available implementations integrate structured MCTS statistics with a local language model to produce grounded, move-level natural language explanations.

**Example output:**
```
Move played: e7e6
Score: -0.011 | Visits: 3

AlphaXplain:
"e7e6 was selected because it supports central control and
 avoids immediate tactical threats. The policy network
 assigned it the highest prior, and MCTS confirmed this
 with the most visits during search."
```

---

## ğŸ“ Mathematical Foundations

Every equation below maps directly to a module in the codebase.

---

### 1. Markov Decision Process (MDP)

The chess environment is formally defined as:

```
(S, A, P, R, Î³)
```

| Symbol | Meaning | Chess Mapping |
|---|---|---|
| `S` | State space | All legal board positions |
| `A` | Action space | All legal moves |
| `P` | Transition function | Deterministic move outcomes |
| `R` | Reward function | +1 win, 0 draw, -1 loss |
| `Î³` | Discount factor | 0.99 â€” values near-future outcomes |

Chess is a **deterministic, fully observable, two-player zero-sum MDP** â€” one of the cleanest environments RL can operate in.

---

### 2. State Encoding

Raw board positions cannot be fed directly to a neural network. I apply an encoding function:

```
Ï†: S â†’ â„^(8Ã—8Ã—12)
```

The 8Ã—8 board is encoded as 12 binary planes â€” one per piece type per color (6 piece types Ã— 2 colors). Additional planes encode castling rights, turn, and move count. This gives the network spatially structured input that convolutions can process.

---

### 3. Action Encoding

All chess moves are mapped to a structured action space:

```
Ïˆ: A â†’ â„^(73Ã—8Ã—8)
```

73 move planes Ã— 64 squares = 4,672 possible actions. Each plane encodes a specific move type (queen moves by direction, knight moves, underpromotions). This lets the policy head output a full distribution over all legal moves in a single forward pass.

---

### 4. Neural Network â€” Policy and Value

The policy and value heads share a deep residual backbone:

```
(p, v) = f_Î¸(s)
```

| Symbol | Meaning |
|---|---|
| `s` | Encoded board state |
| `f_Î¸` | Neural network with learnable parameters Î¸ |
| `p` | Policy vector â€” probability distribution over all moves |
| `v` | Value scalar âˆˆ [-1, +1] â€” predicted game outcome |

#### Convolution (Spatial Pattern Detection)

```
y(i,j) = Î£_{u,v} x(i+u, j+v) Â· k(u,v)
```

Convolutional filters detect spatial patterns â€” pins, forks, open files â€” by sliding learned kernels across the board representation.

#### Residual Block (Deep Network Stability)

```
y = F(x) + x
```

Skip connections allow gradients to flow cleanly through deep networks, preventing vanishing gradients. AlphaZero uses 20â€“40 residual blocks. AlphaXplain uses a reduced version appropriate to available compute.

---

### 5. Monte Carlo Tree Search (MCTS)

MCTS is the search backbone. It builds a game tree guided by the neural network, accumulating visit statistics to identify the strongest moves.

#### PUCT Formula â€” The Heart of AlphaXplain

At each node, the move to explore is selected by:

```
a = argmax_a ( Q(s,a) + c_puct Â· P(s,a) Â· âˆšN(s) / (1 + N(s,a)) )
```

| Symbol | Meaning |
|---|---|
| `Q(s,a)` | Mean value estimate for move `a` from state `s` |
| `P(s,a)` | Prior probability from the neural network |
| `N(s)` | Total visits to state `s` |
| `N(s,a)` | Visits to move `a` from state `s` |
| `c_puct` | Exploration constant (typically 1.0â€“2.5) |

This formula elegantly balances **exploitation** (high Q â€” moves that worked) against **exploration** (high P, low N â€” promising but untested moves). As `N(s,a)` grows, the exploration bonus shrinks, naturally focusing search on the best lines.

#### MCTS Backup Equations

After each simulation reaches a terminal or leaf node, statistics are propagated backwards through the tree:

```
N(s,a) â† N(s,a) + 1
W(s,a) â† W(s,a) + v
Q(s,a) = W(s,a) / N(s,a)
```

`W` accumulates total value; `Q` is its running mean. Over many simulations, `Q` converges to an accurate estimate of move quality â€” and these are the exact values AlphaXplain passes to the LLM for explanation.

---

### 6. Self-Play and Approximate Policy Iteration

AlphaXplain's learning process is a form of **Approximate Policy Iteration (API)**:

```
Ï€_{t+1} â‰ˆ MCTS(Ï€_t)
```

Each generation's policy drives self-play. MCTS improves upon the raw network policy `p` to produce a stronger search policy `Ï€`. The network then learns to imitate `Ï€`, creating a virtuous cycle:

```
Policy â†’ MCTS Search â†’ Better Policy â†’ Train Network â†’ Stronger Policy â†’ Repeat
```

The training dataset is formally defined as:

```
D = { (s_t, Ï€_t, z) }
```

Each sample is a (board state, MCTS policy, game outcome) triple. This is **supervised learning on self-generated data** â€” no human knowledge required at any stage.

---

### 7. Value Function and Bellman Backup

The value function estimates expected return from a state:

```
V(s) = E[ Î£_{t=0}^{âˆ} Î³^t r_t ]
```

The **Bellman backup** â€” the theoretical backbone of all reinforcement learning â€” connects values across time steps:

```
V(s) = E[ r + Î³ V(s') ]
```

For action-values:

```
Q(s,a) = r + Î³ Â· max_a Q(s', a)
```

This recurrence is what allows AlphaXplain to reason about the future. Every value prediction is anchored to actual game outcomes through this chain.

---

### 8. Policy Gradient Theory

The policy improves via the **Policy Gradient Theorem**:

```
âˆ‡_Î¸ J = E[ âˆ‡_Î¸ log Ï€(a|s;Î¸) Â· A(s,a) ]
```

Where the **advantage function** is:

```
A(s,a) = Q(s,a) - V(s)
```

The advantage answers: *was this move better or worse than average in this position?* Positive advantage increases the move's probability; negative decreases it. This is not magic â€” it is the direction of the gradient.

---

### 9. Loss Function

The network minimizes a combined loss over the training dataset:

```
l = (z - v)Â² - Ï€áµ€ log p + câ€–Î¸â€–Â²
```

| Term | Name | Meaning |
|---|---|---|
| `(z - v)Â²` | Value loss | How wrong was the win prediction? |
| `-Ï€áµ€ log p` | Policy loss (cross-entropy) | How different is `p` from the MCTS-refined policy `Ï€`? |
| `câ€–Î¸â€–Â²` | L2 regularization | Prevent overfitting, keep weights bounded |

The cross-entropy term is the engine of self-teaching: the network learns to predict what MCTS recommends, compressing tree search knowledge directly into network weights.

---

### 10. Optimization â€” Adam

Weights are updated via gradient descent:

```
Î¸ â† Î¸ - Î± âˆ‡_Î¸ l
```

In practice, I use **Adam**, which maintains adaptive moment estimates for stable training:

```
m_t = Î²â‚ m_{t-1} + (1 - Î²â‚) g_t          # First moment (mean)
v_t = Î²â‚‚ v_{t-1} + (1 - Î²â‚‚) g_tÂ²         # Second moment (variance)
Î¸_t = Î¸_{t-1} - Î± Â· mÌ‚_t / (âˆšvÌ‚_t + Îµ)   # Bias-corrected update
```

Adam prevents oscillation during training and handles sparse gradients well â€” critical when most board positions appear only once during early self-play.

---

### 11. Exploration â€” Dirichlet Noise

To prevent the root node from collapsing to a single line, I inject noise at the start of each search:

```
P'(s,a) = (1 - Îµ) P(s,a) + Îµ Dir(Î±)
```

| Symbol | Meaning |
|---|---|
| `Îµ` | Noise weight (typically 0.25) |
| `Dir(Î±)` | Dirichlet distribution draw (Î± â‰ˆ 0.3 for chess) |

Without this, the policy becomes deterministic too early â€” a failure mode called **policy collapse**. Dirichlet noise ensures **ergodicity**: every move retains a non-zero probability of being explored, preventing the engine from getting locked into a narrow repertoire.

---

### 12. Convergence and Limitations

AlphaXplain approximates the optimal policy:

```
Ï€* = argmax_Ï€ J(Ï€)
```

Convergence is theoretically guaranteed under infinite compute and perfect function approximation. At this scale, real limitations apply: function approximation error from a small network, insufficient self-play data, distribution shift between training phases, and short MCTS simulation budgets. These are expected consequences of the compute gap â€” not architectural flaws.

---

### 13. Computational Complexity

| Component | Complexity |
|---|---|
| MCTS per move | O(simulations Ã— depth) |
| Network forward pass | O(parameters Ã— board size) |
| Training step | O(batch size Ã— parameters) |
| Full training run | O(iterations Ã— games Ã— moves per game) |

At full AlphaZero scale these numbers become astronomical. AlphaXplain makes deliberate trade-offs at every level to remain runnable on a single consumer machine.

---

## ğŸ” The Full AlphaXplain Loop

```
THINK      (p, v) = f_Î¸(s)                     Network evaluates position
ACT        a ~ Ï€(a|s)                           Sample move from MCTS policy
OBSERVE    (s_t, a_t, r_t, s_{t+1}, z)         Record experience
LEARN      l = (z-v)Â² - Ï€áµ€ log p + câ€–Î¸â€–Â²       Compute combined loss
IMPROVE    Î¸ â† Î¸ - Î±âˆ‡_Î¸ l                       Update weights
EXPLAIN    MCTS stats â†’ LLM â†’ natural language  AlphaXplain's unique step
REPEAT     Î¸â‚€ â†’ Î¸â‚ â†’ Î¸â‚‚ â†’ ...                  Each generation stronger
```

---

## ğŸš€ Future Work

### ğŸŸ¢ 1. Coach Mode
After a game ends, pass the full PGN to the LLM:
> *"Explain my biggest mistake."*

### ğŸŸ¢ 2. Opening Trainer
Auto-detect the opening (Sicilian, French, Ruy LÃ³pez) and explain the strategic plan behind each.

### ğŸŸ¢ 3. Blunder Detector
If Q-value drops sharply after a move:
> *"You blundered because your knight on e5 became undefended."*

### ğŸŸ¢ 4. Chat With AlphaXplain
```
User:          Why didn't you castle?
AlphaXplain:   Castling was deprioritized because the kingside pawns
               were advanced, reducing shelter. MCTS visited it only
               2 times vs 18 for the chosen move.
```

### ğŸŸ¢ 5. Commentary Mode
Real-time narration during play:
> *"White increases pressure on the d-file. Black's queen is running out of squares..."*

---

## ğŸ“ Project Structure

```
alphaxplain/
â”‚
â”œâ”€â”€ agent/          # AI agents and player wrappers
â”œâ”€â”€ env/            # Chess environment (python-chess)
â”œâ”€â”€ mcts/           # Monte Carlo Tree Search + PUCT
â”œâ”€â”€ rl/             # Self-play and training loop
â”œâ”€â”€ llm/            # LLM explanation generator (Ollama + Phi)
â”œâ”€â”€ utils/          # Helper functions and state encoders
â”œâ”€â”€ visual/         # Pygame GUI and board rendering
â”œâ”€â”€ models/         # Saved model checkpoints (.pth)
â””â”€â”€ main.py         # Training entry point
```

Each module is fully isolated. You can swap the neural network, LLM, or search algorithm independently.

---

## ğŸ› ï¸ Tools and Technologies

| Component | Tool |
|---|---|
| Game Logic | `python-chess` |
| Neural Networks | `PyTorch` |
| GUI | `Pygame` |
| Local LLM | `Ollama` + `Phi` |
| Numerics | `NumPy` |
| Language | `Python 3.12` |

**Everything runs locally. No cloud APIs. No internet required after setup.**

---

## â–¶ï¸ Running AlphaXplain

```bash
# Launch the visual chess GUI
python -m visual.chess_gui
```

AlphaXplain will play against you and explain each move in the terminal.

---

## ğŸ“Š Reality Check

| | AlphaXplain | Original AlphaZero |
|---|---|---|
| Hardware | 1Ã— consumer CPU/GPU | Thousands of TPUs |
| Training Time | ~30 minutes | Weeks |
| Iterations | ~30 | Millions |
| Playing Strength | Beginner | Superhuman |
| Architecture | Same âœ… | Same âœ… |
| Explainability | âœ… Yes | âŒ No |

The gap in strength is compute. The architecture is identical in spirit. And AlphaXplain does something the original never did: **it explains itself.**

---

## ğŸ” System Overview

**Environment** â€” Built on `python-chess`. Handles board representation, legal move generation, game termination, and state encoding as Ï†: S â†’ â„^(8Ã—8Ã—12).

**Neural Network** â€” A deep residual network that outputs a policy (move probabilities) and a value (position evaluation). Trained exclusively from self-play â€” no human games, no opening books.

**Monte Carlo Tree Search** â€” Explores future positions using the PUCT formula. Visit statistics are backed up after each simulation and serve as both the move selection mechanism and the raw material for explanation.

**Reinforcement Learning Loop** â€” Self-play generates dataset D = {(s_t, Ï€_t, z)}. The network trains on this data. The model updates. The loop repeats.

**LLM Explanation Layer** â€” MCTS outputs (Q-values, visit counts, policy priors) are extracted as structured data and passed to a local language model. The model generates faithful natural language reasoning grounded in real search statistics â€” AlphaXplain's core contribution.

---

## ğŸ§© Why AlphaXplain Matters

Explainable AI (XAI) is one of the most important open problems in machine learning. Most powerful models â€” including the original AlphaZero â€” are black boxes. You see the output, not the reasoning.

AlphaXplain is a working prototype that bridges that gap: a reinforcement learning engine that extracts its own reasoning from tree search statistics and communicates it in natural language. With stronger compute and training, this architecture scales directly toward a genuinely interpretable game-playing agent.

That combination â€” **RL + MCTS + grounded LLM reasoning** â€” is the contribution.

---

## ğŸ‘¤ Author

Built alone, with curiosity, constrained compute, and the belief that understanding *how* an AI thinks matters as much as *how well* it plays.

---

*"The goal was never to beat Stockfish. The goal was to build something that can tell you why it moved."*

